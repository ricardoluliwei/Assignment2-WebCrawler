How many unique pages did you find? Uniqueness for the purposes of this assignment is ONLY established by the URL,
but discarding the fragment part. So, for example, http://www.ics.uci.edu#aaa and http://www.ics.uci.edu#bbb are the same URL.
Even if you implement additional methods for textual similarity detection, please keep considering the above definition of unique pages
for the purposes of counting the unique pages in this assignment.



What is the longest page in terms of the number of words? (HTML markup doesnâ€™t count as words)



What are the 50 most common words in the entire set of pages?
(Ignore English stop words, which can be found, for example, here (Links to an external site.))
Submit the list of common words ordered by frequency.



How many subdomains did you find in the ics.uci.edu domain? Submit the list of
subdomains ordered alphabetically and the number of unique pages detected in each subdomain.
The content of this list should be lines containing URL, number, for example:
http://vision.ics.uci.edu, 10 (not the actual number here)



1.scraper
 avoid duplicated url, different #fragment with same path will be considered as
 same url

 extract the domain to ensure that we will not get out of the website
 ex. ics.uci.edu
 subdomains are allowed ex. xxx.ics.uci.edu

 read robots.txt, avoid pages that can not be visited

 avoid traps, only crawler certain number of pages in a subdomain
 do it in is_valid, record the number of pages of a subdomain, when the number
 excess a certain number, return invalid for all urls of that subdomain

 avoid low value pages, ex. calender
 do it in extract_next_link, use tokenize,

2.